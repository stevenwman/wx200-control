{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "169680f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "smooth_traj_dir = \"smoothed_trajs\"\n",
        "raw_traj_dir = \"individual_trajs\"\n",
        "\n",
        "# Get all trajectory files in the directories\n",
        "# Collect all .npz files in each directory\n",
        "smooth_traj_files = [os.path.join(smooth_traj_dir, f) for f in os.listdir(smooth_traj_dir) if f.endswith('.npz')]\n",
        "raw_traj_files = [os.path.join(raw_traj_dir, f) for f in os.listdir(raw_traj_dir) if f.endswith('.npz')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "6993894c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys in first file: ['timestamps', 'states', 'actions', 'ee_poses_debug', 'object_pose', 'object_visible', 'aruco_ee_in_world', 'aruco_object_in_world', 'aruco_ee_in_object', 'aruco_object_in_ee', 'aruco_visibility', 'augmented_actions', 'metadata', 'smoothed_aruco_ee_in_world', 'smoothed_aruco_object_in_world', 'smoothed_aruco_ee_in_object', 'smoothed_aruco_object_in_ee']\n",
            "observations (21163, 22)\n",
            "smoothed_observations (21163, 22)\n",
            "actions (21163, 7)\n",
            "rewards (21163,)\n",
            "terminals (21163,)\n",
            "next_observations (21163, 22)\n",
            "masks (21163,)\n"
          ]
        }
      ],
      "source": [
        "# Load the first file, print its keys, then aggregate from all files under those keys.\n",
        "merged_data = {}\n",
        "\n",
        "if len(smooth_traj_files) == 0:\n",
        "    raise RuntimeError(\"No trajectory files found!\")\n",
        "\n",
        "# Get keys from the first file\n",
        "first_file = smooth_traj_files[0]\n",
        "first_data = np.load(first_file, allow_pickle=True)\n",
        "print(\"Keys in first file:\", list(first_data.keys()))\n",
        "for key in first_data.keys():\n",
        "    merged_data[key] = {}\n",
        "\n",
        "# Prepare lists to aggregate per traj and then stack at the end\n",
        "obs_list = []\n",
        "smoothed_obs_list = []\n",
        "actions_list = []\n",
        "rewards_list = []\n",
        "terminals_list = []\n",
        "masks_list = []\n",
        "next_obs_list = []\n",
        "\n",
        "# Now iterate through all files, populate the merged_data dict,\n",
        "# and build arrays for observations, actions, rewards, etc. as we go\n",
        "for filename in smooth_traj_files:\n",
        "    data = np.load(filename, allow_pickle=True)\n",
        "    basename = os.path.basename(filename)\n",
        "    for key in data.keys():\n",
        "        if key not in merged_data:\n",
        "            merged_data[key] = {}\n",
        "        merged_data[key][basename] = data[key]\n",
        "    \n",
        "    # Now process arrays for this trajectory\n",
        "    obj_world = data['aruco_object_in_world']\n",
        "    obj_ee = data['aruco_object_in_ee']\n",
        "    ee_world = data['aruco_ee_in_world']\n",
        "\n",
        "\n",
        "    smoothed_obj_world = data['smoothed_aruco_object_in_world']\n",
        "    smoothed_obj_ee = data['smoothed_aruco_object_in_ee']\n",
        "    smoothed_ee_world = data['smoothed_aruco_ee_in_world']\n",
        "\n",
        "    states = data['states']\n",
        "    actions = data['actions']\n",
        "    n = actions.shape[0]\n",
        "    # Sanity: all arrays have same length n\n",
        "    assert obj_world.shape[0] == n\n",
        "    assert obj_ee.shape[0] == n\n",
        "    assert ee_world.shape[0] == n\n",
        "    assert smoothed_obj_world.shape[0] == n\n",
        "    assert smoothed_obj_ee.shape[0] == n\n",
        "    assert smoothed_ee_world.shape[0] == n\n",
        "    assert states.shape[0] == n\n",
        "\n",
        "    # Each timestep: (obj_world (7), obj_ee (7), ee_world (7), last state dim) -> (22,)\n",
        "    obs = np.concatenate([\n",
        "        obj_world,\n",
        "        obj_ee,\n",
        "        ee_world,\n",
        "        states[:, -1][:, None]\n",
        "    ], axis=1)  # (n,22)\n",
        "    obs_list.append(obs)\n",
        "\n",
        "    smoothed_obs = np.concatenate([\n",
        "        smoothed_obj_world,\n",
        "        smoothed_obj_ee,\n",
        "        smoothed_ee_world,\n",
        "        states[:, -1][:, None]\n",
        "    ], axis=1)  # (n,22)\n",
        "    smoothed_obs_list.append(smoothed_obs)\n",
        "\n",
        "    actions_list.append(actions)\n",
        "    \n",
        "    # Build rewards, terminals, masks\n",
        "    rewards = -np.ones(n, dtype=np.float32)\n",
        "    rewards[-1] = 0\n",
        "    terminals = np.zeros(n, dtype=np.float32)\n",
        "    terminals[-1] = 1\n",
        "    masks = np.ones(n, dtype=np.float32)\n",
        "    masks[-1] = 0\n",
        "\n",
        "    rewards_list.append(rewards)\n",
        "    terminals_list.append(terminals)\n",
        "    masks_list.append(masks)\n",
        "\n",
        "    # For next_observations, slide observations by 1 forward in this trajectory, last step same as last\n",
        "    next_obs = np.empty_like(obs)\n",
        "    next_obs[:-1] = obs[1:]\n",
        "    next_obs[-1] = obs[-1]\n",
        "    next_obs_list.append(next_obs)\n",
        "\n",
        "# Stack all\n",
        "observations = np.concatenate(obs_list, axis=0)  # (N, 22)\n",
        "smoothed_observations = np.concatenate(smoothed_obs_list, axis=0)  # (N, 22)\n",
        "actions = np.concatenate(actions_list, axis=0)\n",
        "rewards = np.concatenate(rewards_list, axis=0)\n",
        "terminals = np.concatenate(terminals_list, axis=0)\n",
        "masks = np.concatenate(masks_list, axis=0)\n",
        "next_observations = np.concatenate(next_obs_list, axis=0)\n",
        "\n",
        "# Save results in merged_data-like dict for downstream code if needed\n",
        "merged_data['observations'] = observations\n",
        "merged_data['smoothed_observations'] = smoothed_observations\n",
        "merged_data['actions_flat'] = actions  # avoid key conflict with dict-of-arrays actions\n",
        "merged_data['rewards'] = rewards\n",
        "merged_data['terminals'] = terminals\n",
        "merged_data['next_observations'] = next_observations\n",
        "merged_data['masks'] = masks\n",
        "\n",
        "# Optionally print shapes for confirmation\n",
        "print(\"observations\", observations.shape)\n",
        "print(\"smoothed_observations\", smoothed_observations.shape)\n",
        "print(\"actions\", actions.shape)\n",
        "print(\"rewards\", rewards.shape)\n",
        "print(\"terminals\", terminals.shape)\n",
        "print(\"next_observations\", next_observations.shape)\n",
        "print(\"masks\", masks.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "802fa43f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "merged_data saved to merged_data.npz\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.savez(\"merged_data.npz\", **merged_data)\n",
        "print(\"merged_data saved to merged_data.npz\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a7c246b",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = '/home/steven/Desktop/work/research/action_chunk_q_learning/envs/hardware_old/merged_dataset.npz'\n",
        "data = np.load(data_path, allow_pickle=True)\n",
        "print(data.files)\n",
        "\n",
        "for key in data.files:\n",
        "    print(f\"{key}: {data[key].shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6544b47a",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = '/home/steven/Desktop/work/research/action_chunk_q_learning/envs/hardware_old/merged_dataset.npz'\n",
        "data = np.load(data_path, allow_pickle=True)\n",
        "print(data.files)\n",
        "\n",
        "for key in data.files:\n",
        "    print(f\"{key}: {data[key].shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57dbbea0",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = '/home/steven/Desktop/work/research/action_chunk_q_learning/envs/hardware_old/merged_dataset.npz'\n",
        "data = np.load(data_path, allow_pickle=True)\n",
        "print(data.files)\n",
        "\n",
        "for key in data.files:\n",
        "    print(f\"{key}: {data[key].shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "831cc380",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['observations', 'actions', 'next_observations', 'rewards', 'terminals', 'masks']\n",
            "observations: (21163, 27)\n",
            "actions: (21163, 7)\n",
            "next_observations: (21163, 27)\n",
            "rewards: (21163,)\n",
            "terminals: (21163,)\n",
            "masks: (21163,)\n"
          ]
        }
      ],
      "source": [
        "data_path = '/home/steven/Desktop/work/research/action_chunk_q_learning/envs/hardware_old/merged_dataset.npz'\n",
        "data = np.load(data_path, allow_pickle=True)\n",
        "print(data.files)\n",
        "\n",
        "for key in data.files:\n",
        "    print(f\"{key}: {data[key].shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "a558c158",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys in first file: ['timestamps', 'states', 'actions', 'ee_poses_debug', 'object_pose', 'object_visible', 'aruco_ee_in_world', 'aruco_object_in_world', 'aruco_ee_in_object', 'aruco_object_in_ee', 'aruco_visibility', 'augmented_actions', 'metadata', 'smoothed_aruco_ee_in_world', 'smoothed_aruco_object_in_world', 'smoothed_aruco_ee_in_object', 'smoothed_aruco_object_in_ee']\n",
            "observations (21163, 20)\n",
            "smoothed_observations (21163, 20)\n",
            "actions (21163, 7)\n",
            "rewards (21163,)\n",
            "terminals (21163,)\n",
            "next_observations (21163, 20)\n",
            "masks (21163,)\n"
          ]
        }
      ],
      "source": [
        "# Load the first file, print its keys, then aggregate from all files under those keys.\n",
        "merged_data_states = {}\n",
        "\n",
        "if len(smooth_traj_files) == 0:\n",
        "    raise RuntimeError(\"No trajectory files found!\")\n",
        "\n",
        "# Get keys from the first file\n",
        "first_file = smooth_traj_files[0]\n",
        "first_data = np.load(first_file, allow_pickle=True)\n",
        "print(\"Keys in first file:\", list(first_data.keys()))\n",
        "for key in first_data.keys():\n",
        "    merged_data_states[key] = {}\n",
        "\n",
        "# Prepare lists to aggregate per traj and then stack at the end\n",
        "obs_list = []\n",
        "smoothed_obs_list = []\n",
        "actions_list = []\n",
        "rewards_list = []\n",
        "terminals_list = []\n",
        "masks_list = []\n",
        "next_obs_list = []\n",
        "\n",
        "# Now iterate through all files, populate the merged_data dict,\n",
        "# and build arrays for observations, actions, rewards, etc. as we go\n",
        "for filename in smooth_traj_files:\n",
        "    data = np.load(filename, allow_pickle=True)\n",
        "    basename = os.path.basename(filename)\n",
        "    for key in data.keys():\n",
        "        if key not in merged_data_states:\n",
        "            merged_data_states[key] = {}\n",
        "        merged_data_states[key][basename] = data[key]\n",
        "    \n",
        "    # Now process arrays for this trajectory\n",
        "    obj_world = data['aruco_object_in_world']\n",
        "    obj_ee = data['aruco_object_in_ee']\n",
        "    ee_world = data['aruco_ee_in_world']\n",
        "\n",
        "    smoothed_obj_world = data['smoothed_aruco_object_in_world']\n",
        "    smoothed_obj_ee = data['smoothed_aruco_object_in_ee']\n",
        "    smoothed_ee_world = data['smoothed_aruco_ee_in_world']\n",
        "    \n",
        "    ee_poses_debug = data['ee_poses_debug']\n",
        "    \n",
        "    states = data['states']\n",
        "    actions = data['actions']\n",
        "    n = actions.shape[0]\n",
        "    # Sanity: all arrays have same length n\n",
        "    assert obj_world.shape[0] == n\n",
        "    assert obj_ee.shape[0] == n\n",
        "    assert ee_world.shape[0] == n\n",
        "    assert smoothed_obj_world.shape[0] == n\n",
        "    assert smoothed_obj_ee.shape[0] == n\n",
        "    assert smoothed_ee_world.shape[0] == n\n",
        "    assert states.shape[0] == n\n",
        "\n",
        "    # Each timestep\n",
        "    obs = np.concatenate([\n",
        "        obj_world,\n",
        "        ee_poses_debug,\n",
        "        states\n",
        "    ], axis=1)\n",
        "    obs_list.append(obs)\n",
        "\n",
        "    smoothed_obs = np.concatenate([\n",
        "        smoothed_obj_world,\n",
        "        ee_poses_debug,\n",
        "        states\n",
        "    ], axis=1) \n",
        "    smoothed_obs_list.append(smoothed_obs)\n",
        "\n",
        "    actions_list.append(actions)\n",
        "    \n",
        "    # Build rewards, terminals, masks\n",
        "    rewards = -np.ones(n, dtype=np.float32)\n",
        "    rewards[-1] = 0\n",
        "    terminals = np.zeros(n, dtype=np.float32)\n",
        "    terminals[-1] = 1\n",
        "    masks = np.ones(n, dtype=np.float32)\n",
        "    masks[-1] = 0\n",
        "\n",
        "    rewards_list.append(rewards)\n",
        "    terminals_list.append(terminals)\n",
        "    masks_list.append(masks)\n",
        "\n",
        "    # For next_observations, slide observations by 1 forward in this trajectory, last step same as last\n",
        "    next_obs = np.empty_like(obs)\n",
        "    next_obs[:-1] = obs[1:]\n",
        "    next_obs[-1] = obs[-1]\n",
        "    next_obs_list.append(next_obs)\n",
        "\n",
        "# Stack all\n",
        "observations = np.concatenate(obs_list, axis=0)  # (N, 22)\n",
        "smoothed_observations = np.concatenate(smoothed_obs_list, axis=0)  # (N, 22)\n",
        "actions = np.concatenate(actions_list, axis=0)\n",
        "rewards = np.concatenate(rewards_list, axis=0)\n",
        "terminals = np.concatenate(terminals_list, axis=0)\n",
        "masks = np.concatenate(masks_list, axis=0)\n",
        "next_observations = np.concatenate(next_obs_list, axis=0)\n",
        "\n",
        "# Save results in merged_data-like dict for downstream code if needed\n",
        "merged_data_states['observations'] = observations\n",
        "merged_data_states['smoothed_observations'] = smoothed_observations\n",
        "merged_data_states['actions_flat'] = actions  # avoid key conflict with dict-of-arrays actions\n",
        "merged_data_states['rewards'] = rewards\n",
        "merged_data_states['terminals'] = terminals\n",
        "merged_data_states['next_observations'] = next_observations\n",
        "merged_data_states['masks'] = masks\n",
        "\n",
        "# Optionally print shapes for confirmation\n",
        "print(\"observations\", observations.shape)\n",
        "print(\"smoothed_observations\", smoothed_observations.shape)\n",
        "print(\"actions\", actions.shape)\n",
        "print(\"rewards\", rewards.shape)\n",
        "print(\"terminals\", terminals.shape)\n",
        "print(\"next_observations\", next_observations.shape)\n",
        "print(\"masks\", masks.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "3b5bae45",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "merged_data_states saved to merged_data_states.npz\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.savez(\"merged_data_states.npz\", **merged_data_states)\n",
        "print(\"merged_data_states saved to merged_data_states.npz\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ogpo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
